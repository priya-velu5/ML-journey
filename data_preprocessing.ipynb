{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # useful for preprocessing\n",
    "import numpy as np # has numpy arrays \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a pandas dataframe \n",
    "dataset = pd.read_csv(\"Data.csv\") # automatically assumes 1st row is header \n",
    "\n",
    "#create a matrix of features and dependent variables\n",
    "'''\n",
    "features are the independant variables which will be used to predict the outcome of something( the dependent variable)\n",
    "in the given dataset: \n",
    "country, salary and age are features. \n",
    "purchases is the dependent variable ( it is dependant on the features)\n",
    "\n",
    "X is the feature vector matirx = 1st 3 columns\n",
    "Y is the dependent variable matrix = last column\n",
    "=> just split the dataframe and extract. \n",
    "iloc = locate index. \n",
    "'''\n",
    "\n",
    "x= dataset.iloc[:,:-1].values\n",
    "y= dataset.iloc[:,-1] .values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Various approaches : \n",
    "1. vv less missing data - just delete those rows\n",
    "2. replace by avg of that column \n",
    "3. replace by median\n",
    "4. replace by highest frequency value\n",
    "\n",
    "Scikit learn is the amaze library that we're gonna use \n",
    "'''\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# missing values are denoted by nan and replace it by its mean\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')  \n",
    "\n",
    "# fit method inputs the data to the object. \n",
    "'''\n",
    "fit method expects only numerical data. So data in any other format should be converted to numerical data. \n",
    "in our case only cols 2 and 3 have missing values and they are in numberical format already. \n",
    "'''\n",
    "imputer.fit(x[:,1:3])\n",
    "# transform performs the operation and fills the empty cells. \n",
    "x[:,1:3] = imputer.transform(x[:,1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the goal here is to encode the categorical data. There are only 3 countries. So we can call france = 0, spain = 1 and\n",
    "germany = 3. But the ML model might consider this to be some sort of a heirarchy and give wrong results. \n",
    "This is where one hot encoding comes in. We will make every country as a bool = does it belong to france or not and etc. \n",
    "As a result we're splitting one country col to 3 different cols aka, giving every country a unique ID \n",
    "\n",
    "for eg: France,44,72000 - this becomes:\n",
    "\n",
    "France Spain Germany Age   Salary  \n",
    "    1   0     0       44   72000     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# what kind of tranformation, which index \n",
    "# tranformer = [(what type of transformation, transformation class, columns to be transformed)]\n",
    "# if remainder isnt given, the remaining columns will not appear in the final result. \n",
    "# We want the other columns of the matrix to be left the way it is. \n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])] , remainder ='passthrough')\n",
    "\n",
    "# fit and tranform \n",
    "# convert the matrix to a numpy array. \n",
    "x= np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert yes and no to 0s and 1s\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "\n",
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "\n",
      "[0 1 0 0 1 1 0 1]\n",
      "\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train_test_split(matrix of features, dependent variable,test_size=, random_state = an int)\n",
    "#random_state specifies the seed value for the randomised splitting of data into test and train sets. \n",
    "#if you make this a constant, the test and train split will always have the same random split every time. \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(x_train)\n",
    "print()\n",
    "print(x_test)\n",
    "print()\n",
    "print(y_train)\n",
    "print()\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is done to ensure some variables dont dominate others. \n",
    "for eg, In multi variable linear regression model: y = b0 + b1x1+ b2x2 +b3x3 +.... bnxn \n",
    "If x3's values are too high but the weightage of every variable is the same. Hence we can scale x3 down by adjusting its coefficient. \n",
    "\n",
    "There are two kinds of feature scaling : Normalisation and Standardisation\n",
    "\n",
    "Standardisation makes all the values to be contained in the same range.\n",
    "\n",
    "Xstand = [x - mean(x)]/standard_deviation(x)\n",
    "Xnorm = [x-min(x)]/[max(x) - min(x)]\n",
    "\n",
    "Standardisation works all the time, whereas normalisation works well only when features have a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "sc = StandardScaler()\n",
    "'''\n",
    "Dont do standarisation on the categorical variables. Theyre dummy variables to denote a country. \n",
    "'''\n",
    "x_train[:,3:]= sc.fit_transform(x_train[:,3:])\n",
    "\n",
    "#we will not calculate the new scaling factor for train dataset. We're only scaling to the same factor as before. \n",
    "# so dont do fit on test. only transform. \n",
    "\n",
    "x_test[:,3:] = sc.transform(x_test[:,3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "\n",
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print()\n",
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxDRfLvKVBN9HjXcmlURF3",
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
